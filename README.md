# üìö | C√°lculo II - M√©todos N√∫mericos
## Sobre o reposit√≥rio
Esse reposit√≥rio cont√©m os projetos realizados durante a disciplina de C√°lculo II Honors. O objetivo √© ir al√©m da teoria e observar na pr√°tica o funcionamento de alguns m√©todos num√©ricos utilizando python e bibliotecas como o numpy e o matplotlib.



---
### Projeto 1: Integra√ß√£o num√©rica
Esse projeto consiste na implementa√ß√£o de m√©todos para a aproxima√ß√£o da integral pelo m√©todo num√©rico, no caso, a **Regra do Trap√©zio** e a **Regra de Simpson**.

---

### Projeto 2: Diferen√ßas finitas 
Esse projeto consiste na implementa√ß√£o de duas fun√ß√µes, uma para resolver numericamente uma EDO (equa√ß√£o diferencial ordin√°ria) com a condi√ß√£o inicial na forma y'= f(x,y), y(x0) = y0 e outra para analisar como a escolha do "passo" impacta no erro das **diferen√ßas finitas avan√ßadas**, assim calculando o erro entre a derivada por diferen√ßas finitas avan√ßadas e a derivada real. Ambos geram gr√°ficos no matplotlib, segue exemplos:



**Ode Solver**

Input: ode solver(lambda x,y: 10*np.sqrt(y)*np.sin(x)+x,0,0,100,500,True 

![ode_solver](https://github.com/user-attachments/assets/ea33e992-7889-4973-bcf8-999ec85d1917)

**Erro nas Diferen√ßas finitas**

Input: fd error(lambda x: np.atan(x),lambda x:1/(1+x**2),1,1e-15,1e-1,100)

![fd_error](https://github.com/user-attachments/assets/d07fcc00-c73e-423a-9104-79cb7c019f3f)

---

### Projeto 3: Otimiza√ß√£o
Esse projeto tem o objetivo de implementar v√°rios m√©todos para otimiza√ß√£o e outros auxiliares. Os m√©todos implementados foram:
  - **Derivada Parcial com Diferen√ßas Finitas Centradas**
  - **Busca Linear*
  - **M√©todo do Gradiente**
  - **M√©todo de Newton**
  - Salvaguardas para o M√©todo de Newton
    - Matriz singular
    - Manter a dire√ß√£o de descida
Segue exemplo:


Input: 

def f(x):
    return np.log(np.exp(-x[0])) + np.exp(-x[1]) + np.exp(x[0]+x[1])

def f_grid(X, Y):
    return np.log(np.exp(-X)) + np.exp(-Y) + np.exp(X + Y)

def hess(x):
    return np.array([[np.exp(x[0]+x[1]), np.exp(x[0]+x[1])],[np.exp(x[0]+x[1]),np.exp(-x[1])+np.exp(x[0]+x[1])]])

def grad(x):
    return np.array([ -1 + np.exp(x[0]+x[1]), -np.exp(-x[1]) + np.exp(x[0] + x[1]) ])

x,k = gd(f,np.array([2.0,-0.5]),grad,1e-5, 0.1, 10000,  True, 1e-5,  True, True)
print(f"x = {x}")
print(f"k = {k}")

x,k = newton(f,np.array([2,-0.5]),grad,hess,1e-5, 0.1, 10000, True, 1e-5,  True, True)
print(f"x = {x}")
print(f"k = {k}")

**Minimiza√ß√£o pelo M√©todo do Gradiente**

![metodo_gradiente](https://github.com/user-attachments/assets/4e0132f0-363d-48ac-b018-41fb9d68f311)


**Minimiza√ß√£o pelo M√©todo de Newton**

![metodo_newton](https://github.com/user-attachments/assets/5e334bfb-ac09-4769-ad0a-827213fea587)


---
